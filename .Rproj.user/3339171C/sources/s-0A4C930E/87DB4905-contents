---
title: "Khai-Nguyen Nguyen"
description:
  Personal Distill website of Khai-Nguyen Nguyen for Math230. The description is extracted from my [personal website](https://nkn002.github.io/).
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)

# Learn more about creating websites with Distill at:
# https://rstudio.github.io/distill/website.html

# Learn more about publishing to GitHub Pages at:
# https://rstudio.github.io/distill/publish_website.html#github-pages

```

```{r, out.width = "400px"}
knitr::include_graphics("my_face.jpeg")
```

About
===
I am a CS and Engineering student at Bucknell University at Lewisburg, Pennsylvania. I am particularly interested in methods of improving existing models' learning to reach human-level language understanding. To this end, I hope to do research on the **interpretability and explainability** and **representation learning** of NLP models.

I am fortunate to be advised by Professor Alex Kelly (Carleton University) and Professor Thiago Serra (Bucknell University) in my previous research.


My work
===
What does it mean for a model to achieve human-level language understanding, and how do we get there? While the first part of the question is more philosophical, I believe one direction toward the answer to the second part lies in **adopting elements from human language learning** into models. Toward this end, my research projects in NLP have explored ([1](https://arxiv.org/abs/2210.05487)) using visually grounded information to learn better representation in multilingual context - a technique motivated by the multimodal learning in humans - and ([2](https://github.com/toontran/limitless-sequence-modeling)) summarizing past texts and incorporate them to the current input to capture long-range dependencies - a phenomenom utilized by writers and story-tellers in their works. 

Towards the answer to the first question, I believe interpretability is of great importance. Many models are said to perform certain tasks at the same level as humans, if not better. However, what attributed to their success? What is the rationale behind their outputs? If we are able to answer these questions, we are one step closer to understand the capabilities of the state-of-the-arts, and subsequently could work toward improving them. My related experience on this topic mainly deals with feedforward networks, but I am exploring ways to extending the methods developed with Professor Serra to the transformer architecture.


CS education
===
I am also interested in work on activities related to education in CS. I have been a teaching assistant for CS2: Data Structures and Algorithms for Bucknell's CS Department for two semesters, which I really enjoyed. I also did research on the identifying which topics are important in CS2 and which are difficult. I hope to be a TA in my graduate studies.


Contact
===
You can view my CV [here](https://drive.google.com/file/d/1_JP0uLViGp4pLtpqtSFBK8w2Ozj1iiRX/view?usp=sharing). Please feel free to contact me via [email](nkn002@bucknell.edu)!

Updates
===
**April, 2023** <br>
Our Important and Difficult Topics in CS2 paper was accepted to ASEE 2023!

**March, 2023** <br>
Our Getting away paper was accepted to ICLR SNN 2023!

**Feb, 2023** <br>
Our Getting away paper was accepted to CPAIOR 2023! <br>
Our Like a bilingual baby paper is now under submission to CogSci 2023



Papers
====
**Getting away with more network pruning: From sparsity to geometry and linear regions** <br>
Jeffrey Cai, **Khai-Nguyen Nguyen**, Nishant Shrestha, Aidan Good, Ruisen Tu, Xin Yu, Shandian Zhe, Thiago Serra <br>
_Accepted to CPAIOR 2023_ <br>
**Note:** co-first authored

**Like a bilingual baby: The advantage of visually grounding a bilingual language model** <br>
**Khai-Nguyen Nguyen**, Zixin Tang, Ankur Mali, M Alex Kelly<br>
_arxiv_

**Important and Difficult Topics in CS2: An Expert Consensus via Delphi Study** <br>
Lea Wittie, Anastasia Kurdia, Meriel Huggard, **Khai-Nguyen Nguyen** <br>
_Accepted to the ASEE Annual Conference and Exposition 2023_



